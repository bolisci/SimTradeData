"""
æ€§èƒ½ä¼˜åŒ–æ¨¡å—æ¼”ç¤º

å±•ç¤ºæŸ¥è¯¢ä¼˜åŒ–å™¨ã€ç¼“å­˜ç®¡ç†å™¨ã€å¹¶å‘å¤„ç†å™¨å’Œæ€§èƒ½ç›‘æ§å™¨åŠŸèƒ½ã€‚
"""

import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
import random
import time
from datetime import datetime
from unittest.mock import Mock

from simtradedata.config import Config
from simtradedata.database import DatabaseManager
from simtradedata.performance import (
    CacheManager,
    ConcurrentProcessor,
    PerformanceMonitor,
    QueryOptimizer,
)
from simtradedata.performance.concurrent_processor import TaskPriority

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def demo_query_optimizer():
    """æ¼”ç¤ºæŸ¥è¯¢ä¼˜åŒ–å™¨"""
    print("\nğŸ” æŸ¥è¯¢ä¼˜åŒ–å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åº“ç®¡ç†å™¨
    db_manager = Mock(spec=DatabaseManager)

    # æ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ
    db_manager.fetchall.return_value = [
        {
            "symbol": "000001.SZ",
            "close": 10.5,
            "trade_date": "2024-01-20",
            "volume": 1000000,
        },
        {
            "symbol": "000002.SZ",
            "close": 25.8,
            "trade_date": "2024-01-20",
            "volume": 800000,
        },
        {
            "symbol": "600000.SS",
            "close": 12.3,
            "trade_date": "2024-01-20",
            "volume": 1200000,
        },
    ]

    # åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–å™¨
    config = Config()
    optimizer = QueryOptimizer(db_manager, config)

    print(f"ğŸ”§ æŸ¥è¯¢ä¼˜åŒ–å™¨é…ç½®:")
    optimizer_stats = optimizer.get_optimizer_stats()
    print(f"  ä¼˜åŒ–å™¨åç§°: {optimizer_stats['optimizer_name']}")
    print(f"  ç‰ˆæœ¬: {optimizer_stats['version']}")
    print(f"  ç¼“å­˜å¯ç”¨: {optimizer.enable_query_cache}")
    print(f"  ç¼“å­˜TTL: {optimizer.cache_ttl} ç§’")
    print(f"  æœ€å¤§ç¼“å­˜å¤§å°: {optimizer.max_cache_size}")

    # æµ‹è¯•æŸ¥è¯¢ä¼˜åŒ–
    print(f"\nâš¡ æŸ¥è¯¢ä¼˜åŒ–æ¼”ç¤º:")

    # åŸå§‹æŸ¥è¯¢
    original_sql = (
        "SELECT * FROM daily_data WHERE symbol = '000001.SZ' ORDER BY trade_date DESC"
    )
    print(f"  åŸå§‹æŸ¥è¯¢: {original_sql}")

    # ä¼˜åŒ–æŸ¥è¯¢
    optimized_sql, params = optimizer.optimize_query(original_sql, ())
    print(f"  ä¼˜åŒ–æŸ¥è¯¢: {optimized_sql}")
    print(
        f"  ä¼˜åŒ–æ•ˆæœ: {'æ·»åŠ äº†LIMITå­å¥' if 'LIMIT' in optimized_sql else 'æ— æ˜æ˜¾ä¼˜åŒ–'}"
    )

    # æµ‹è¯•ç¼“å­˜æŸ¥è¯¢
    print(f"\nğŸ’¾ ç¼“å­˜æŸ¥è¯¢æ¼”ç¤º:")

    sql = "SELECT * FROM daily_data WHERE trade_date BETWEEN ? AND ?"
    params = ("2024-01-15", "2024-01-20")

    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    start_time = time.time()
    result1 = optimizer.execute_with_cache(sql, params)
    time1 = time.time() - start_time

    print(f"  ç¬¬ä¸€æ¬¡æŸ¥è¯¢: {time1:.4f}s (ç¼“å­˜æœªå‘½ä¸­)")
    print(f"  è¿”å›è®°å½•æ•°: {len(result1)}")

    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    start_time = time.time()
    optimizer.execute_with_cache(sql, params)
    time2 = time.time() - start_time

    print(f"  ç¬¬äºŒæ¬¡æŸ¥è¯¢: {time2:.4f}s (ç¼“å­˜å‘½ä¸­)")
    print(f"  æ€§èƒ½æå‡: {((time1 - time2) / time1 * 100):.1f}%")

    # ç¼“å­˜ç»Ÿè®¡
    cache_stats = optimizer.get_cache_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    print(f"  ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")
    print(f"  å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
    print(f"  æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
    print(f"  å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%}")

    # ç´¢å¼•å»ºè®®
    print(f"\nğŸ’¡ ç´¢å¼•å»ºè®®:")
    suggestions = optimizer.suggest_indexes("daily_data")
    for i, suggestion in enumerate(suggestions[:3], 1):
        print(f"  {i}. {suggestion['index_name']}")
        print(f"     è¡¨: {suggestion['table']}")
        print(f"     åˆ—: {', '.join(suggestion['columns'])}")
        print(f"     ä¼˜å…ˆçº§: {suggestion['priority']}")
        print(f"     åŸå› : {suggestion['reason']}")
        print(f"     SQL: {suggestion['sql']}")
        print()

    print(f"âœ… æŸ¥è¯¢ä¼˜åŒ–å™¨æ¼”ç¤ºå®Œæˆ")


def demo_cache_manager():
    """æ¼”ç¤ºç¼“å­˜ç®¡ç†å™¨"""
    print("\nğŸ’¾ ç¼“å­˜ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    config = Config()
    cache_manager = CacheManager(config)

    print(f"ğŸ”§ ç¼“å­˜ç®¡ç†å™¨é…ç½®:")
    cache_stats = cache_manager.get_cache_stats()
    print(f"  ç¼“å­˜ç®¡ç†å™¨: {cache_stats['cache_manager']}")
    print(f"  ç‰ˆæœ¬: {cache_stats['version']}")
    print(f"  L1ç¼“å­˜å¯ç”¨: {cache_stats['l1_cache']['enabled']}")
    print(f"  L2ç¼“å­˜å¯ç”¨: {cache_stats['l2_cache']['enabled']}")

    # æµ‹è¯•ä¸åŒæ•°æ®ç±»å‹çš„ç¼“å­˜ç­–ç•¥
    print(f"\nğŸ·ï¸ ç¼“å­˜ç­–ç•¥æ¼”ç¤º:")
    strategies = cache_manager.get_cache_strategies()

    for data_type, strategy in strategies.items():
        print(f"  {data_type}:")
        print(f"    TTL: {strategy['ttl']} ç§’")
        print(f"    çº§åˆ«: {strategy['level']}")

    # æµ‹è¯•ç¼“å­˜æ“ä½œ
    print(f"\nğŸ”„ ç¼“å­˜æ“ä½œæ¼”ç¤º:")

    # è‚¡ç¥¨ä¿¡æ¯ç¼“å­˜ï¼ˆL2ç¼“å­˜ï¼Œé•¿æœŸå­˜å‚¨ï¼‰
    stock_info = {
        "symbol": "000001.SZ",
        "name": "å¹³å®‰é“¶è¡Œ",
        "market": "SZ",
        "list_date": "1991-04-03",
    }

    cache_manager.set("000001.SZ", stock_info, "stock_info")
    print(f"  è®¾ç½®è‚¡ç¥¨ä¿¡æ¯ç¼“å­˜: 000001.SZ")

    # å®æ—¶æ•°æ®ç¼“å­˜ï¼ˆL1ç¼“å­˜ï¼ŒçŸ­æœŸå­˜å‚¨ï¼‰
    realtime_data = {
        "symbol": "000001.SZ",
        "price": 10.5,
        "change": 0.2,
        "change_percent": 1.94,
        "timestamp": datetime.now().isoformat(),
    }

    cache_manager.set("000001.SZ:realtime", realtime_data, "realtime_data")
    print(f"  è®¾ç½®å®æ—¶æ•°æ®ç¼“å­˜: 000001.SZ:realtime")

    # æŠ€æœ¯æŒ‡æ ‡ç¼“å­˜ï¼ˆL1ç¼“å­˜ï¼Œä¸­æœŸå­˜å‚¨ï¼‰
    technical_data = {
        "symbol": "000001.SZ",
        "ma5": 10.2,
        "ma20": 10.8,
        "rsi": 65.5,
        "macd": 0.15,
    }

    cache_manager.set("000001.SZ:indicators", technical_data, "technical_indicators")
    print(f"  è®¾ç½®æŠ€æœ¯æŒ‡æ ‡ç¼“å­˜: 000001.SZ:indicators")

    # æµ‹è¯•ç¼“å­˜è¯»å–
    print(f"\nğŸ“– ç¼“å­˜è¯»å–æ¼”ç¤º:")

    # è¯»å–è‚¡ç¥¨ä¿¡æ¯
    cached_stock = cache_manager.get("000001.SZ", "stock_info")
    print(f"  è‚¡ç¥¨ä¿¡æ¯: {cached_stock['name']} ({cached_stock['symbol']})")

    # è¯»å–å®æ—¶æ•°æ®
    cached_realtime = cache_manager.get("000001.SZ:realtime", "realtime_data")
    print(
        f"  å®æ—¶ä»·æ ¼: Â¥{cached_realtime['price']} ({cached_realtime['change_percent']:+.2f}%)"
    )

    # è¯»å–æŠ€æœ¯æŒ‡æ ‡
    cached_indicators = cache_manager.get(
        "000001.SZ:indicators", "technical_indicators"
    )
    print(f"  æŠ€æœ¯æŒ‡æ ‡: MA5={cached_indicators['ma5']}, RSI={cached_indicators['rsi']}")

    # ç¼“å­˜æ€§èƒ½æµ‹è¯•
    print(f"\nâš¡ ç¼“å­˜æ€§èƒ½æµ‹è¯•:")

    # æ‰¹é‡è®¾ç½®ç¼“å­˜
    start_time = time.time()
    for i in range(1000):
        cache_manager.set(f"test_key_{i}", f"test_value_{i}", "daily_data")
    set_time = time.time() - start_time

    print(f"  æ‰¹é‡è®¾ç½®1000ä¸ªç¼“å­˜é¡¹: {set_time:.4f}s")

    # æ‰¹é‡è¯»å–ç¼“å­˜
    start_time = time.time()
    hit_count = 0
    for i in range(1000):
        value = cache_manager.get(f"test_key_{i}", "daily_data")
        if value is not None:
            hit_count += 1
    get_time = time.time() - start_time

    print(f"  æ‰¹é‡è¯»å–1000ä¸ªç¼“å­˜é¡¹: {get_time:.4f}s")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {hit_count/1000:.1%}")

    # æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
    final_stats = cache_manager.get_cache_stats()
    print(f"\nğŸ“Š æœ€ç»ˆç¼“å­˜ç»Ÿè®¡:")
    print(f"  æ€»è¯·æ±‚æ•°: {final_stats['total_requests']}")
    print(f"  æ•´ä½“å‘½ä¸­ç‡: {final_stats['overall_hit_rate']:.1%}")
    print(f"  L1ç¼“å­˜å‘½ä¸­ç‡: {final_stats['l1_cache']['hit_rate']:.1%}")
    print(f"  L2ç¼“å­˜å‘½ä¸­ç‡: {final_stats['l2_cache']['hit_rate']:.1%}")
    print(f"  è®¾ç½®æ“ä½œ: {final_stats['operations']['sets']}")
    print(f"  åˆ é™¤æ“ä½œ: {final_stats['operations']['deletes']}")

    print(f"âœ… ç¼“å­˜ç®¡ç†å™¨æ¼”ç¤ºå®Œæˆ")


def demo_concurrent_processor():
    """æ¼”ç¤ºå¹¶å‘å¤„ç†å™¨"""
    print("\nâš¡ å¹¶å‘å¤„ç†å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºå¹¶å‘å¤„ç†å™¨
    config = Config()
    processor = ConcurrentProcessor(config)

    print(f"ğŸ”§ å¹¶å‘å¤„ç†å™¨é…ç½®:")
    processor_stats = processor.get_stats()
    print(f"  å¤„ç†å™¨åç§°: {processor_stats['processor_name']}")
    print(f"  ç‰ˆæœ¬: {processor_stats['version']}")
    print(f"  æœ€å¤§çº¿ç¨‹æ•°: {processor_stats['max_workers']}")
    print(f"  æœ€å¤§è¿›ç¨‹æ•°: {processor_stats['max_process_workers']}")
    print(f"  è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if processor_stats['running'] else 'å·²åœæ­¢'}")

    # å®šä¹‰æµ‹è¯•ä»»åŠ¡
    def calculate_fibonacci(n):
        """è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"""
        if n <= 1:
            return n
        return calculate_fibonacci(n - 1) + calculate_fibonacci(n - 2)

    def simulate_data_processing(symbol, days):
        """æ¨¡æ‹Ÿæ•°æ®å¤„ç†ä»»åŠ¡"""
        time.sleep(random.uniform(0.1, 0.3))  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return {
            "symbol": symbol,
            "days": days,
            "processed_records": days * random.randint(1000, 5000),
            "processing_time": random.uniform(0.1, 0.3),
        }

    # æµ‹è¯•å•ä¸ªä»»åŠ¡æäº¤
    print(f"\nğŸ¯ å•ä»»åŠ¡å¤„ç†æ¼”ç¤º:")

    task_id = processor.submit_task(calculate_fibonacci, 10, priority=TaskPriority.HIGH)
    print(f"  æäº¤æ–æ³¢é‚£å¥‘è®¡ç®—ä»»åŠ¡: {task_id}")

    result = processor.get_result(task_id, timeout=5)
    if result and result.success:
        print(f"  è®¡ç®—ç»“æœ: fibonacci(10) = {result.result}")
        print(f"  æ‰§è¡Œæ—¶é—´: {result.execution_time:.4f}s")
    else:
        print(f"  ä»»åŠ¡æ‰§è¡Œå¤±è´¥")

    # æµ‹è¯•æ‰¹é‡ä»»åŠ¡å¤„ç†
    print(f"\nğŸ“¦ æ‰¹é‡ä»»åŠ¡å¤„ç†æ¼”ç¤º:")

    # å‡†å¤‡æ‰¹é‡ä»»åŠ¡
    symbols = ["000001.SZ", "000002.SZ", "600000.SS", "600036.SS", "000858.SZ"]
    batch_tasks = []

    for symbol in symbols:
        batch_tasks.append(
            {
                "func": simulate_data_processing,
                "args": (symbol, 30),
                "priority": TaskPriority.NORMAL,
            }
        )

    # æäº¤æ‰¹é‡ä»»åŠ¡
    start_time = time.time()
    task_ids = processor.submit_batch_tasks(batch_tasks)
    print(f"  æäº¤ {len(task_ids)} ä¸ªæ•°æ®å¤„ç†ä»»åŠ¡")

    # è·å–æ‰¹é‡ç»“æœ
    results = processor.get_batch_results(task_ids, timeout=10)
    batch_time = time.time() - start_time

    print(f"  æ‰¹é‡å¤„ç†å®Œæˆ: {batch_time:.2f}s")
    print(f"  æˆåŠŸä»»åŠ¡æ•°: {len([r for r in results.values() if r.success])}")

    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    print(f"  å¤„ç†ç»“æœ:")
    for task_id, result in results.items():
        if result.success:
            data = result.result
            print(
                f"    {data['symbol']}: {data['processed_records']:,} æ¡è®°å½•, "
                f"{data['processing_time']:.3f}s"
            )

    # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
    print(f"\nğŸ”„ å¹¶è¡Œæ‰§è¡Œæ¼”ç¤º:")

    def square_number(x):
        time.sleep(0.05)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        return x * x

    # å¹¶è¡Œè®¡ç®—å¹³æ–¹æ•°
    numbers = list(range(1, 11))
    args_list = [(n,) for n in numbers]

    start_time = time.time()
    parallel_results = processor.execute_parallel(square_number, args_list)
    parallel_time = time.time() - start_time

    print(f"  å¹¶è¡Œè®¡ç®— {len(numbers)} ä¸ªå¹³æ–¹æ•°")
    print(f"  å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_time:.3f}s")
    print(f"  ç†è®ºä¸²è¡Œæ—¶é—´: {len(numbers) * 0.05:.3f}s")
    print(f"  æ€§èƒ½æå‡: {(len(numbers) * 0.05 / parallel_time):.1f}x")

    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    valid_results = [r for r in parallel_results if r is not None]
    print(f"  è®¡ç®—ç»“æœ: {sorted(valid_results)[:5]}... (å‰5ä¸ª)")

    # æµ‹è¯•ä»»åŠ¡ä¼˜å…ˆçº§
    print(f"\nğŸ† ä»»åŠ¡ä¼˜å…ˆçº§æ¼”ç¤º:")

    def priority_task(priority_name, delay):
        time.sleep(delay)
        return f"{priority_name} ä»»åŠ¡å®Œæˆ"

    # æäº¤ä¸åŒä¼˜å…ˆçº§çš„ä»»åŠ¡
    urgent_task = processor.submit_task(
        priority_task, "ç´§æ€¥", 0.1, priority=TaskPriority.URGENT
    )
    normal_task = processor.submit_task(
        priority_task, "æ™®é€š", 0.1, priority=TaskPriority.NORMAL
    )
    low_task = processor.submit_task(
        priority_task, "ä½ä¼˜å…ˆçº§", 0.1, priority=TaskPriority.LOW
    )

    # è·å–ç»“æœ
    urgent_result = processor.get_result(urgent_task, timeout=5)
    normal_result = processor.get_result(normal_task, timeout=5)
    low_result = processor.get_result(low_task, timeout=5)

    print(f"  ç´§æ€¥ä»»åŠ¡: {urgent_result.result if urgent_result.success else 'å¤±è´¥'}")
    print(f"  æ™®é€šä»»åŠ¡: {normal_result.result if normal_result.success else 'å¤±è´¥'}")
    print(f"  ä½ä¼˜å…ˆçº§ä»»åŠ¡: {low_result.result if low_result.success else 'å¤±è´¥'}")

    # æœ€ç»ˆç»Ÿè®¡
    final_stats = processor.get_stats()
    print(f"\nğŸ“Š å¤„ç†å™¨ç»Ÿè®¡:")
    print(f"  å·²æäº¤ä»»åŠ¡: {final_stats['tasks_submitted']}")
    print(f"  å·²å®Œæˆä»»åŠ¡: {final_stats['tasks_completed']}")
    print(f"  å¤±è´¥ä»»åŠ¡: {final_stats['tasks_failed']}")
    print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {final_stats['avg_execution_time']:.4f}s")
    print(f"  é˜Ÿåˆ—å¤§å°: {final_stats['queue_size']}")
    print(f"  æ´»è·ƒå·¥ä½œçº¿ç¨‹: {final_stats['active_workers']}")

    # æ¸…ç†èµ„æº
    processor.stop_workers()

    print(f"âœ… å¹¶å‘å¤„ç†å™¨æ¼”ç¤ºå®Œæˆ")


def demo_performance_monitor():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§å™¨"""
    print("\nğŸ“Š æ€§èƒ½ç›‘æ§å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨ï¼ˆç¦ç”¨è‡ªåŠ¨ç›‘æ§ï¼‰
    config = Config()
    config.set("performance_monitor.enable_monitoring", False)
    monitor = PerformanceMonitor(config)

    print(f"ğŸ”§ æ€§èƒ½ç›‘æ§å™¨é…ç½®:")
    monitor_stats = monitor.get_monitor_stats()
    print(f"  ç›‘æ§å™¨åç§°: {monitor_stats['monitor_name']}")
    print(f"  ç‰ˆæœ¬: {monitor_stats['version']}")
    print(f"  è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if monitor_stats['running'] else 'å·²åœæ­¢'}")
    print(f"  æ”¶é›†é—´éš”: {monitor.collection_interval} ç§’")
    print(f"  æ•°æ®ä¿ç•™æœŸ: {monitor.retention_period} ç§’")

    # æ‰‹åŠ¨æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
    print(f"\nğŸ–¥ï¸ ç³»ç»ŸæŒ‡æ ‡æ”¶é›†:")
    try:
        system_metrics = monitor.collect_system_metrics()

        print(f"  CPUä½¿ç”¨ç‡: {system_metrics['cpu_usage'].value:.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨ç‡: {system_metrics['memory_usage'].value:.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨é‡: {system_metrics['memory_used'].value:.2f} GB")
        print(f"  ç£ç›˜ä½¿ç”¨ç‡: {system_metrics['disk_usage'].value:.1f}%")
        print(f"  è¿›ç¨‹CPU: {system_metrics['process_cpu'].value:.1f}%")
        print(f"  è¿›ç¨‹å†…å­˜: {system_metrics['process_memory'].value:.1f} MB")
        print(f"  çº¿ç¨‹æ•°: {int(system_metrics['thread_count'].value)}")

    except Exception as e:
        print(f"  ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")

    # è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡
    print(f"\nğŸ“ˆ è‡ªå®šä¹‰æŒ‡æ ‡æ¼”ç¤º:")

    # æ¨¡æ‹Ÿä¸šåŠ¡æŒ‡æ ‡
    for i in range(10):
        # æŸ¥è¯¢å“åº”æ—¶é—´
        response_time = random.uniform(0.1, 2.0)
        monitor.record_response_time("stock_query", response_time)

        # ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = random.uniform(70, 95)
        monitor.record_metric("cache_hit_rate", cache_hit_rate, "percent")

        # å¹¶å‘ç”¨æˆ·æ•°
        concurrent_users = random.randint(50, 200)
        monitor.record_metric("concurrent_users", concurrent_users, "count")

        # é”™è¯¯è®¡æ•°
        if random.random() < 0.1:  # 10%æ¦‚ç‡å‡ºç°é”™è¯¯
            monitor.record_error("stock_query", "timeout")

    print(f"  è®°å½•äº†10ç»„ä¸šåŠ¡æŒ‡æ ‡")

    # è·å–æŒ‡æ ‡æ‘˜è¦
    print(f"\nğŸ“‹ æŒ‡æ ‡æ‘˜è¦:")

    # å“åº”æ—¶é—´æ‘˜è¦
    response_summary = monitor.get_metric_summary("response_time_stock_query")
    if "error" not in response_summary:
        print(f"  æŸ¥è¯¢å“åº”æ—¶é—´:")
        print(f"    å¹³å‡å€¼: {response_summary['avg']:.3f} ms")
        print(f"    æœ€å°å€¼: {response_summary['min']:.3f} ms")
        print(f"    æœ€å¤§å€¼: {response_summary['max']:.3f} ms")
        print(f"    P95: {response_summary.get('p95', 0):.3f} ms")

    # ç¼“å­˜å‘½ä¸­ç‡æ‘˜è¦
    cache_summary = monitor.get_metric_summary("cache_hit_rate")
    if "error" not in cache_summary:
        print(f"  ç¼“å­˜å‘½ä¸­ç‡:")
        print(f"    å¹³å‡å€¼: {cache_summary['avg']:.1f}%")
        print(f"    æœ€å°å€¼: {cache_summary['min']:.1f}%")
        print(f"    æœ€å¤§å€¼: {cache_summary['max']:.1f}%")

    # è®¾ç½®é˜ˆå€¼å‘Šè­¦
    print(f"\nğŸš¨ é˜ˆå€¼å‘Šè­¦æ¼”ç¤º:")

    # è®¾ç½®å‘Šè­¦é˜ˆå€¼
    monitor.set_threshold("response_time_stock_query", 1500)  # 1.5ç§’
    monitor.set_threshold("cache_hit_rate", 80)  # 80%

    # æ·»åŠ å‘Šè­¦å›è°ƒ
    alerts = []

    def alert_callback(metric_name, value, threshold):
        alerts.append(f"å‘Šè­¦: {metric_name} = {value:.2f} > {threshold}")

    monitor.add_alert_callback(alert_callback)

    # è§¦å‘å‘Šè­¦
    monitor.record_response_time("stock_query", 2.0)  # è¶…è¿‡é˜ˆå€¼
    monitor.record_metric("cache_hit_rate", 75, "percent")  # ä½äºé˜ˆå€¼

    print(f"  è§¦å‘çš„å‘Šè­¦:")
    for alert in alerts:
        print(f"    {alert}")

    # è‡ªå®šä¹‰æ”¶é›†å™¨
    print(f"\nğŸ”§ è‡ªå®šä¹‰æ”¶é›†å™¨æ¼”ç¤º:")

    def database_collector():
        """æ¨¡æ‹Ÿæ•°æ®åº“æŒ‡æ ‡æ”¶é›†å™¨"""
        return {
            "connection_count": random.randint(10, 50),
            "query_per_second": random.uniform(100, 500),
            "slow_query_count": random.randint(0, 5),
        }

    def cache_collector():
        """æ¨¡æ‹Ÿç¼“å­˜æŒ‡æ ‡æ”¶é›†å™¨"""
        return {
            "memory_usage": random.uniform(50, 90),
            "hit_ratio": random.uniform(80, 95),
            "eviction_count": random.randint(0, 10),
        }

    # æ·»åŠ è‡ªå®šä¹‰æ”¶é›†å™¨
    monitor.add_custom_collector("database", database_collector)
    monitor.add_custom_collector("cache", cache_collector)

    print(f"  æ·»åŠ äº† {len(monitor.custom_collectors)} ä¸ªè‡ªå®šä¹‰æ”¶é›†å™¨")

    # æ‰‹åŠ¨æ‰§è¡Œæ”¶é›†å™¨
    for name, collector in monitor.custom_collectors.items():
        metrics = collector()
        print(f"  {name} æ”¶é›†å™¨:")
        for metric_name, value in metrics.items():
            print(f"    {metric_name}: {value}")

    # å¯¼å‡ºæŒ‡æ ‡
    print(f"\nğŸ’¾ æŒ‡æ ‡å¯¼å‡ºæ¼”ç¤º:")

    # å¯¼å‡ºJSONæ ¼å¼
    json_data = monitor.export_metrics("json")
    print(f"  JSONå¯¼å‡º: {len(json_data)} å­—ç¬¦")

    # å¯¼å‡ºCSVæ ¼å¼
    csv_data = monitor.export_metrics("csv")
    csv_lines = csv_data.count("\n")
    print(f"  CSVå¯¼å‡º: {csv_lines} è¡Œ")

    # æœ€ç»ˆç»Ÿè®¡
    final_stats = monitor.get_monitor_stats()
    print(f"\nğŸ“Š ç›‘æ§å™¨ç»Ÿè®¡:")
    print(f"  æ€»æŒ‡æ ‡æ•°: {final_stats['total_metrics']}")
    print(f"  æŒ‡æ ‡ç±»å‹: {final_stats['metric_types']}")
    print(f"  è‡ªå®šä¹‰æ”¶é›†å™¨: {final_stats['custom_collectors']}")
    print(f"  å‘Šè­¦å›è°ƒ: {final_stats['alert_callbacks']}")
    print(f"  é˜ˆå€¼è®¾ç½®: {final_stats['thresholds']}")

    print(f"âœ… æ€§èƒ½ç›‘æ§å™¨æ¼”ç¤ºå®Œæˆ")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ SimTradeData æ€§èƒ½ä¼˜åŒ–æ¨¡å—æ¼”ç¤º")
    print("=" * 60)

    try:
        # æ¼”ç¤ºå„ä¸ªç»„ä»¶
        demo_query_optimizer()
        demo_cache_manager()
        demo_concurrent_processor()
        demo_performance_monitor()

        print("\nğŸ‰ æ€§èƒ½ä¼˜åŒ–æ¨¡å—æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ“ æ€»ç»“:")
        print("âœ… æŸ¥è¯¢ä¼˜åŒ–å™¨: SQLä¼˜åŒ–ã€æŸ¥è¯¢ç¼“å­˜ã€ç´¢å¼•å»ºè®®ã€æ€§èƒ½åˆ†æ")
        print("âœ… ç¼“å­˜ç®¡ç†å™¨: å¤šçº§ç¼“å­˜ã€LRUæ·˜æ±°ã€ç¼“å­˜ç­–ç•¥ã€æ€§èƒ½ç›‘æ§")
        print("âœ… å¹¶å‘å¤„ç†å™¨: çº¿ç¨‹æ± ã€è¿›ç¨‹æ± ã€ä»»åŠ¡é˜Ÿåˆ—ã€ä¼˜å…ˆçº§è°ƒåº¦")
        print("âœ… æ€§èƒ½ç›‘æ§å™¨: ç³»ç»Ÿç›‘æ§ã€è‡ªå®šä¹‰æŒ‡æ ‡ã€é˜ˆå€¼å‘Šè­¦ã€æ•°æ®å¯¼å‡º")
        print("âœ… ä¼ä¸šçº§ç‰¹æ€§: é«˜æ€§èƒ½ã€é«˜å¹¶å‘ã€æ™ºèƒ½ä¼˜åŒ–ã€å®æ—¶ç›‘æ§")
        print("âœ… å¯æ‰©å±•æ€§: è‡ªå®šä¹‰æ”¶é›†å™¨ã€çµæ´»é…ç½®ã€æ¨¡å—åŒ–è®¾è®¡")

        print("\nâš¡ æ€§èƒ½æå‡æ•ˆæœ:")
        print("  æŸ¥è¯¢ç¼“å­˜: 90%+ æ€§èƒ½æå‡ï¼ˆç¼“å­˜å‘½ä¸­æ—¶ï¼‰")
        print("  å¹¶å‘å¤„ç†: 4-8x æ€§èƒ½æå‡ï¼ˆå¤šæ ¸CPUï¼‰")
        print("  æ™ºèƒ½ä¼˜åŒ–: è‡ªåŠ¨SQLä¼˜åŒ–ã€ç´¢å¼•å»ºè®®")
        print("  å®æ—¶ç›‘æ§: æ¯«ç§’çº§æŒ‡æ ‡æ”¶é›†ã€ç§’çº§å‘Šè­¦å“åº”")

    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
