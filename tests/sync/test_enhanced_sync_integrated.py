"""
æµ‹è¯•å¢å¼ºçš„åŒæ­¥æµç¨‹
æ•´åˆäº†åŸæ¥çš„ test_sync_enhanced.py å’Œ test_sync_incremental.py çš„åŠŸèƒ½
"""

from datetime import date

import pytest

from tests.conftest import BaseTestClass, SyncTestMixin


@pytest.mark.sync
class TestEnhancedSync(BaseTestClass, SyncTestMixin):
    """æµ‹è¯•å¢å¼ºçš„åŒæ­¥æµç¨‹å’Œå¢é‡åŒæ­¥"""

    def test_enhanced_processing_engine(self, processing_engine, db_manager):
        """æµ‹è¯•æ•°æ®å¤„ç†å¼•æ“çš„å¢å¼ºåŠŸèƒ½"""
        # ä½¿ç”¨æ ‡å‡†æµ‹è¯•æ•°æ®
        test_symbol = "000001.SZ"
        start_date, end_date = self.get_test_date_range()

        self.print_test_info(
            "å¢å¼ºæ•°æ®å¤„ç†å¼•æ“æµ‹è¯•", [test_symbol], start_date, end_date
        )
        self.clean_test_data(db_manager, [test_symbol], str(start_date), str(end_date))

        # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“å¤„ç†æ•°æ®
        result = processing_engine.process_symbol_data(
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            force_update=True,
        )

        # éªŒè¯å¤„ç†ç»“æœ
        assert result.get(
            "success", False
        ), f"æ•°æ®å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"

        # éªŒè¯æ•°æ®åº“ä¸­çš„ç»“æœ
        self._verify_enhanced_data(db_manager, test_symbol, start_date, end_date)

    def test_incremental_sync_with_enhanced_processing(
        self, incremental_sync, db_manager
    ):
        """æµ‹è¯•å¢é‡åŒæ­¥å™¨ä½¿ç”¨å¢å¼ºçš„æ•°æ®å¤„ç†å¼•æ“"""
        test_symbols = self.get_test_symbols()[:2]  # ä½¿ç”¨å‰ä¸¤ä¸ªæµ‹è¯•è‚¡ç¥¨
        target_date = date(2025, 1, 24)

        self.print_test_info(
            "å¢é‡åŒæ­¥å¢å¼ºå¤„ç†æµ‹è¯•", test_symbols, date(2025, 1, 20), target_date
        )
        self.setup_sync_test(db_manager, test_symbols)

        # æ‰§è¡Œå¢é‡åŒæ­¥
        sync_result = incremental_sync.sync_all_symbols(
            target_date=target_date, symbols=test_symbols, frequencies=["1d"]
        )

        self.verify_sync_result(sync_result)

        # éªŒè¯æ¯ä¸ªè‚¡ç¥¨çš„å¢å¼ºæ•°æ®
        for symbol in test_symbols:
            self._verify_enhanced_data_for_symbol(db_manager, symbol)

        # éªŒè¯æ•´ä½“æ•°æ®è´¨é‡
        self._verify_overall_data_quality(db_manager, test_symbols)

    def _verify_enhanced_data(
        self, db_manager, symbol: str, start_date: date, end_date: date
    ):
        """éªŒè¯æ•°æ®å¤„ç†çš„è´¨é‡"""
        sql = """
        SELECT symbol, date, close, change_percent, prev_close, amplitude, 
               source, quality_score, is_limit_up, is_limit_down
        FROM market_data 
        WHERE symbol = ? AND date >= ? AND date <= ?
        ORDER BY date
        """

        records = db_manager.fetchall(sql, (symbol, str(start_date), str(end_date)))

        assert len(records) > 0, f"æœªæ‰¾åˆ°è‚¡ç¥¨ {symbol} çš„æ•°æ®"
        print(f"æ‰¾åˆ° {len(records)} æ¡è®°å½•")

        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€æ¡è®°å½•çš„å†…å®¹
        if records:
            first_record = records[0]
            print(f"ç¬¬ä¸€æ¡è®°å½•å­—æ®µ: {dict(first_record)}")

        # éªŒè¯åŸºç¡€æ•°æ®è´¨é‡
        for record in records:
            # éªŒè¯å¿…éœ€å­—æ®µ
            assert record["symbol"] == symbol, f"è‚¡ç¥¨ä»£ç ä¸åŒ¹é…: {record['symbol']}"
            assert record["date"] is not None, "æ—¥æœŸå­—æ®µä¸èƒ½ä¸ºç©º"
            assert (
                record["close"] is not None and record["close"] > 0
            ), f"æ”¶ç›˜ä»·å¼‚å¸¸: {record['close']}"

        # æ ¹æ®æ•°æ®æºç±»å‹éªŒè¯ç›¸åº”çš„å¤„ç†è´¨é‡
        enhanced_records = [r for r in records if r["source"] == "processed_enhanced"]
        basic_records = [r for r in records if r["source"] == "processed"]

        if enhanced_records:
            # å¦‚æœæœ‰å¢å¼ºå¤„ç†çš„è®°å½•ï¼ŒéªŒè¯è¡ç”Ÿå­—æ®µ
            has_derived_fields = any(
                record["change_percent"] is not None and record["change_percent"] != 0
                for record in enhanced_records
            )
            assert has_derived_fields, "å¢å¼ºå¤„ç†çš„è®°å½•åº”è¯¥æœ‰è®¡ç®—çš„è¡ç”Ÿå­—æ®µ"
            print("âœ… å¢å¼ºå¤„ç†è®°å½•éªŒè¯é€šè¿‡")
        elif basic_records:
            # å¦‚æœåªæœ‰åŸºç¡€å¤„ç†çš„è®°å½•ï¼ŒéªŒè¯åŸºç¡€å­—æ®µå®Œæ•´æ€§
            print(f"âœ… åŸºç¡€å¤„ç†è®°å½•éªŒè¯é€šè¿‡ ({len(basic_records)} æ¡è®°å½•)")
        else:
            pytest.fail("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å¤„ç†è®°å½•")

        print("âœ… æ•°æ®å¤„ç†è´¨é‡éªŒè¯é€šè¿‡")

    def _verify_enhanced_data_for_symbol(self, db_manager, symbol: str):
        """éªŒè¯å•ä¸ªè‚¡ç¥¨çš„æ•°æ®å¤„ç†è´¨é‡"""
        sql = """
        SELECT date, close, change_percent, prev_close, amplitude, source
        FROM market_data 
        WHERE symbol = ? AND date >= '2025-01-20'
        ORDER BY date DESC
        LIMIT 3
        """

        records = db_manager.fetchall(sql, (symbol,))

        if not records:
            pytest.fail(f"è‚¡ç¥¨ {symbol} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•")

        enhanced_count = sum(1 for r in records if r["source"] == "processed_enhanced")
        basic_count = sum(1 for r in records if r["source"] == "processed")
        derived_field_count = sum(
            1
            for r in records
            if r["change_percent"] is not None and r["change_percent"] != 0
        )

        print(
            f"è‚¡ç¥¨ {symbol}: å¢å¼ºå¤„ç†è®°å½• {enhanced_count}/{len(records)}, "
            f"åŸºç¡€å¤„ç†è®°å½• {basic_count}/{len(records)}, "
            f"è¡ç”Ÿå­—æ®µè®°å½• {derived_field_count}/{len(records)}"
        )

        # éªŒè¯è‡³å°‘æœ‰æœ‰æ•ˆçš„æ•°æ®å¤„ç†ï¼ˆå¢å¼ºæˆ–åŸºç¡€ï¼‰
        assert (
            enhanced_count + basic_count
        ) > 0, f"è‚¡ç¥¨ {symbol} æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¤„ç†è®°å½•"

    def _verify_overall_data_quality(self, db_manager, symbols: list):
        """éªŒè¯æ•´ä½“æ•°æ®è´¨é‡"""
        placeholders = ",".join("?" * len(symbols))
        quality_sql = f"""
        SELECT 
            COUNT(*) as total_records,
            COUNT(CASE WHEN source = 'processed_enhanced' THEN 1 END) as enhanced_records,
            COUNT(CASE WHEN source = 'processed' THEN 1 END) as basic_records,
            COUNT(CASE WHEN change_percent IS NOT NULL AND change_percent != 0 THEN 1 END) as derived_field_records,
            AVG(CASE WHEN change_percent IS NOT NULL THEN change_percent ELSE 0 END) as avg_change_percent
        FROM market_data 
        WHERE symbol IN ({placeholders}) AND date >= '2025-01-20'
        """

        quality_result = db_manager.fetchone(quality_sql, tuple(symbols))

        if not quality_result:
            pytest.fail("æ— æ³•è·å–æ•°æ®è´¨é‡ç»Ÿè®¡")

        total = quality_result["total_records"]
        enhanced = quality_result["enhanced_records"]
        basic = quality_result["basic_records"]
        derived = quality_result["derived_field_records"]

        print(f"\n=== æ•´ä½“æ•°æ®è´¨é‡ç»Ÿè®¡ ===")
        print(f"æ€»è®°å½•æ•°: {total}")
        if total > 0:
            print(f"å¢å¼ºå¤„ç†è®°å½•: {enhanced} ({enhanced/total*100:.1f}%)")
            print(f"åŸºç¡€å¤„ç†è®°å½•: {basic} ({basic/total*100:.1f}%)")
            print(f"è¡ç”Ÿå­—æ®µè®°å½•: {derived} ({derived/total*100:.1f}%)")

        # éªŒè¯æ•°æ®è´¨é‡æ ‡å‡†
        assert total > 0, "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®è®°å½•"
        assert (enhanced + basic) >= total * 0.8, "æœ‰æ•ˆå¤„ç†è®°å½•æ¯”ä¾‹è¿‡ä½ï¼ˆåº”è¯¥ >= 80%ï¼‰"

        if enhanced > 0:
            print("ğŸ‰ å‘ç°å¢å¼ºå¤„ç†è®°å½•ï¼")
            assert derived > 0, "å¢å¼ºå¤„ç†è®°å½•åº”è¯¥åŒ…å«è¡ç”Ÿå­—æ®µ"
        else:
            print("â„¹ï¸ å½“å‰ä½¿ç”¨åŸºç¡€å¤„ç†æ¨¡å¼")

        print("âœ… æ•´ä½“æ•°æ®è´¨é‡éªŒè¯é€šè¿‡ï¼")


@pytest.mark.sync
@pytest.mark.integration
class TestSyncIntegration(BaseTestClass):
    """åŒæ­¥åŠŸèƒ½é›†æˆæµ‹è¯•"""

    def test_full_sync_pipeline(self, processing_engine, incremental_sync, db_manager):
        """æµ‹è¯•å®Œæ•´çš„åŒæ­¥æµæ°´çº¿"""
        test_symbols = self.get_test_symbols()[:1]  # ä½¿ç”¨ä¸€ä¸ªæµ‹è¯•è‚¡ç¥¨
        start_date, end_date = self.get_test_date_range()

        self.print_test_info("å®Œæ•´åŒæ­¥æµæ°´çº¿æµ‹è¯•", test_symbols, start_date, end_date)
        self.clean_test_data(db_manager, test_symbols)

        # æ­¥éª¤1: ä½¿ç”¨å¤„ç†å¼•æ“è¿›è¡Œåˆå§‹æ•°æ®å¤„ç†
        for symbol in test_symbols:
            result = processing_engine.process_symbol_data(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                force_update=True,
            )
            assert result.get("success", False), f"åˆå§‹æ•°æ®å¤„ç†å¤±è´¥: {symbol}"

        # æ­¥éª¤2: ä½¿ç”¨å¢é‡åŒæ­¥è¿›è¡Œåç»­æ›´æ–°
        sync_result = incremental_sync.sync_all_symbols(
            target_date=end_date, symbols=test_symbols, frequencies=["1d"]
        )

        # éªŒè¯å¢é‡åŒæ­¥ç»“æœ - æ ¹æ®å®é™…è¿”å›æ ¼å¼è°ƒæ•´
        print(f"åŒæ­¥ç»“æœ: {sync_result}")
        assert (
            sync_result.get("success_count", 0) > 0
            or sync_result.get("total_symbols", 0) > 0
        ), "å¢é‡åŒæ­¥å¤±è´¥"

        # éªŒè¯æœ€ç»ˆæ•°æ®è´¨é‡
        for symbol in test_symbols:
            assert self.verify_data_exists(
                db_manager, symbol, 3
            ), f"è‚¡ç¥¨ {symbol} æ•°æ®ä¸è¶³"

        print("âœ… å®Œæ•´åŒæ­¥æµæ°´çº¿æµ‹è¯•é€šè¿‡")
